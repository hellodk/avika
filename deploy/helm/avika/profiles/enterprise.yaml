# =============================================================================
# Enterprise Scaling Profile
# =============================================================================
# Target workload:
#   - 100+ NGINX agents
#   - 100,000 requests/second aggregate
#   - 10+ minute sustained load
#   - ~60 million log entries per 10-minute window
#
# Infrastructure requirements:
#   - Kubernetes cluster with 16+ CPU cores available
#   - 32+ GB RAM available across nodes
#   - Fast SSD storage (NVMe recommended)
#   - Low-latency network between pods
#
# Usage:
#   helm install avika ./deploy/helm/avika \
#     -f ./deploy/helm/avika/profiles/enterprise.yaml \
#     --set postgresql.auth.password=<password> \
#     --set clickhouse.password=<password>
# =============================================================================

# -----------------------------------------------------------------------------
# GATEWAY - Scale for 100+ concurrent agent connections
# -----------------------------------------------------------------------------
gateway:
  # 5 replicas for high availability and load distribution
  # Each handles ~20 agents with headroom
  replicaCount: 5
  
  resources:
    limits:
      cpu: 2              # 2 CPU cores per replica
      memory: 2Gi         # 2GB memory for buffers
    requests:
      cpu: 500m
      memory: 512Mi
  
  config:
    # Disable rate limiting for enterprise throughput
    enableRateLimit: false
    rateLimitRPS: 0
    # Extended shutdown for graceful drain
    shutdownTimeout: 60s
    # gRPC tuning for high concurrency
    grpcMaxConcurrentStreams: 200
    grpcMaxRecvMsgSize: 33554432     # 32MB
    grpcMaxSendMsgSize: 33554432     # 32MB
  
  # Enable HPA for dynamic scaling
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 60
    targetMemoryUtilizationPercentage: 70
  
  # Go runtime tuning for high throughput
  env:
    GOGC: "100"                # Standard GC (memory available)
    GOMEMLIMIT: "1800MiB"      # Below container limit
    GOMAXPROCS: "4"            # Use 4 cores
  
  # Pod disruption budget
  podDisruptionBudget:
    enabled: true
    minAvailable: 2
  
  # Anti-affinity to spread across nodes
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                component: gateway
            topologyKey: kubernetes.io/hostname

# -----------------------------------------------------------------------------
# CLICKHOUSE - High-throughput time-series ingestion
# -----------------------------------------------------------------------------
# Target: 100k log entries/second sustained
# Storage: ~60M entries/10min * 500 bytes = ~30GB per 10-minute window
# -----------------------------------------------------------------------------
clickhouse:
  enabled: true
  
  persistence:
    size: 100Gi          # 100GB for extended retention
    storageClass: ""     # Use default or specify fast SSD class
  
  resources:
    limits:
      cpu: 8             # 8 CPU cores
      memory: 16Gi       # 16GB RAM
    requests:
      cpu: 4
      memory: 8Gi
  
  # ClickHouse server configuration
  config:
    # Increased buffer sizes for high throughput
    logBufferSize: 500000        # 500k log buffer
    spanBufferSize: 1000000      # 1M span buffer
    metricsBufferSize: 50000     # 50k metrics buffer
    
    # Larger batch sizes for efficient writes
    logBatchSize: 50000          # Flush every 50k logs
    spanBatchSize: 100000        # Flush every 100k spans
    flushIntervalMs: 50          # 50ms flush interval
  
  # ClickHouse server settings (via ConfigMap)
  serverConfig: |
    <clickhouse>
      <!-- Memory limits -->
      <max_server_memory_usage_to_ram_ratio>0.8</max_server_memory_usage_to_ram_ratio>
      <max_memory_usage>12000000000</max_memory_usage>
      
      <!-- Connection limits -->
      <max_connections>1000</max_connections>
      <max_concurrent_queries>200</max_concurrent_queries>
      
      <!-- Background processing -->
      <background_pool_size>16</background_pool_size>
      <background_schedule_pool_size>16</background_schedule_pool_size>
      <background_merges_mutations_concurrency_ratio>2</background_merges_mutations_concurrency_ratio>
      
      <!-- Write optimization -->
      <merge_tree>
        <max_bytes_to_merge_at_max_space_in_pool>161061273600</max_bytes_to_merge_at_max_space_in_pool>
        <max_bytes_to_merge_at_min_space_in_pool>1048576</max_bytes_to_merge_at_min_space_in_pool>
        <number_of_free_entries_in_pool_to_lower_max_size_of_merge>8</number_of_free_entries_in_pool_to_lower_max_size_of_merge>
      </merge_tree>
      
      <!-- Async inserts for high throughput -->
      <async_insert>1</async_insert>
      <async_insert_threads>8</async_insert_threads>
      <async_insert_max_data_size>10000000</async_insert_max_data_size>
      <async_insert_busy_timeout_ms>50</async_insert_busy_timeout_ms>
      <wait_for_async_insert>0</wait_for_async_insert>
    </clickhouse>

# -----------------------------------------------------------------------------
# POSTGRESQL - Metadata storage (agents, configs)
# -----------------------------------------------------------------------------
# Moderate load: agent registrations, config changes
# Not on hot path for 100k RPS logs
# -----------------------------------------------------------------------------
postgresql:
  enabled: true
  
  primary:
    persistence:
      enabled: true
      size: 20Gi
    
    resources:
      limits:
        cpu: 2
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi
    
    # PostgreSQL tuning
    extendedConfiguration: |
      # Connection handling
      max_connections = 500
      superuser_reserved_connections = 10
      
      # Memory
      shared_buffers = 512MB
      work_mem = 16MB
      maintenance_work_mem = 256MB
      effective_cache_size = 1GB
      
      # WAL
      wal_buffers = 16MB
      checkpoint_completion_target = 0.9
      
      # Query planning
      random_page_cost = 1.1
      effective_io_concurrency = 200

# -----------------------------------------------------------------------------
# REDPANDA - Message queue for async processing
# -----------------------------------------------------------------------------
# Used for log aggregation pipeline
# -----------------------------------------------------------------------------
redpanda:
  enabled: true
  replicaCount: 3      # 3-node cluster for HA
  
  resources:
    limits:
      cpu: 4
      memory: 8Gi
    requests:
      cpu: 2
      memory: 4Gi
  
  config:
    # Topic configuration
    logRetentionMs: 3600000          # 1 hour retention
    logRetentionBytes: 53687091200   # 50GB per partition
    logSegmentBytes: 1073741824      # 1GB segments
    
    # Performance tuning
    numPartitions: 12                # Partitions for parallelism
    replicationFactor: 2             # 2x replication
    
    # Producer settings
    producerLingerMs: 5              # Batch for 5ms
    producerBatchSize: 1048576       # 1MB batches

# -----------------------------------------------------------------------------
# OTEL COLLECTOR - Log/trace pipeline
# -----------------------------------------------------------------------------
otelCollector:
  enabled: true
  replicaCount: 2      # 2 replicas for HA
  
  resources:
    limits:
      cpu: 2
      memory: 2Gi
    requests:
      cpu: 500m
      memory: 512Mi

# -----------------------------------------------------------------------------
# AGENTS - Resource limits per agent
# -----------------------------------------------------------------------------
agent:
  resources:
    limits:
      cpu: 500m         # Allow burst for log parsing
      memory: 128Mi     # Increased for high log volume
    requests:
      cpu: 100m
      memory: 64Mi
  
  env:
    GOGC: "100"
    GOMEMLIMIT: "100MiB"
    GOMAXPROCS: "2"

# -----------------------------------------------------------------------------
# FRONTEND - Dashboard
# -----------------------------------------------------------------------------
frontend:
  replicaCount: 2
  
  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 512Mi

# -----------------------------------------------------------------------------
# MONITORING - Prometheus scraping
# -----------------------------------------------------------------------------
monitoring:
  prometheus:
    enabled: true
    scrape: true
    interval: 15s      # More frequent for high-throughput visibility
    path: /metrics

# -----------------------------------------------------------------------------
# POD SECURITY
# -----------------------------------------------------------------------------
podSecurityContext:
  runAsNonRoot: false
  fsGroup: 1000

securityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: false
  capabilities:
    drop:
      - ALL
